{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7bdde8-87bf-4210-ac2f-027939c51f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15825f2a-21c1-4acb-b745-8b2f7fe502c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_date_range(start_date: str, end_date: str):\n",
    "    \"\"\"Generate list of dates from start_date to end_date inclusive.\"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    date_list = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        date_list.append(current.strftime(\"%Y-%m-%d\"))\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "    return date_list\n",
    "\n",
    "# Example usage\n",
    "dates = get_date_range(\"2025-06-29\", \"2025-07-2\")\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003d119-64fe-4cdd-b9f5-0ac18692f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, output_dir):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=30) as response:\n",
    "            response.raise_for_status()\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")\n",
    "\n",
    "def download_for_date(date, output_dir):\n",
    "    url_file = f\"https://data.binance.vision/data/spot/daily/trades/BTCUSDT/BTCUSDT-trades-{date}.zip\"\n",
    "    url_checksum = url_file + \".CHECKSUM\"\n",
    "    \n",
    "    download_file(url_file, output_dir)\n",
    "    download_file(url_checksum, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a600d6f-27aa-494a-a1e6-ac003b35d500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Params\n",
    "download_dir = \"binance_data\"\n",
    "dates = get_date_range(\"2017-08-17\", \"2025-06-30\")\n",
    "\n",
    "# Use ThreadPoolExecutor to run multiple downloads in parallel\n",
    "max_workers = 12  # Adjust based on your network and system\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(download_for_date, date, download_dir) for date in dates]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        future.result()  # This will re-raise any exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fcabc-1854-4b47-aa89-69006ea1abb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d2038-3e7a-49ff-a93e-f6681d3472d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c5f8a-889a-46bd-bd51-e28c651d2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When i was scraping sequentially\n",
    "\n",
    "# for date in dates:\n",
    "\n",
    "#     # Replace with your actual .zip URL\n",
    "#     url_file = f\"https://data.binance.vision/data/spot/daily/trades/BTCUSDT/BTCUSDT-trades-{date}.zip\"\n",
    "#     url_checksum = url_file + \".CHECKSUM\"\n",
    "    \n",
    "#     filename = url_file.split(\"/\")[-1]  # Extracts filename from URL\n",
    "#     file_path = os.path.join(download_dir, filename)\n",
    "    \n",
    "#     # Download the file\n",
    "#     with requests.get(url_file, stream=True) as response:\n",
    "#         response.raise_for_status()  # Raise an error for bad status codes\n",
    "#         with open(file_path, \"wb\") as f:\n",
    "#             for chunk in response.iter_content(chunk_size=8192):\n",
    "#                 f.write(chunk)\n",
    "    \n",
    "#     print(f\"Downloaded: {filename}\")\n",
    "\n",
    "#     # Download the checksum\n",
    "#     filename = url_checksum.split(\"/\")[-1]  # Extracts filename from URL\n",
    "#     file_path = os.path.join(download_dir, filename)\n",
    "    \n",
    "#     with requests.get(url_checksum, stream=True) as response:\n",
    "#         response.raise_for_status()  # Raise an error for bad status codes\n",
    "#         with open(file_path, \"wb\") as f:\n",
    "#             for chunk in response.iter_content(chunk_size=8192):\n",
    "#                 f.write(chunk)\n",
    "    \n",
    "#     print(f\"Downloaded: {filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
